<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">2 posts tagged with &quot;LMM&quot; | AutoGen</title><meta data-react-helmet="true" property="og:title" content="2 posts tagged with &quot;LMM&quot; | AutoGen"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io/autogen/blog/tags/lmm"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/autogen/img/ag.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io/autogen/blog/tags/lmm"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/tags/lmm" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/tags/lmm" hreflang="x-default"><link rel="stylesheet" href="/autogen/assets/css/styles.a35b243d.css">
<link rel="preload" href="/autogen/assets/js/runtime~main.5370b1fb.js" as="script">
<link rel="preload" href="/autogen/assets/js/main.690578dc.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">AutoGen</b></a><a class="navbar__item navbar__link" href="/autogen/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/autogen/docs/reference/agentchat/conversable_agent">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/autogen/blog">Blog</a><a class="navbar__item navbar__link" href="/autogen/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/09/EcoAssistant">EcoAssistant - Using LLM Assistants More Accurately and Affordably</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/10/26/TeachableAgent">AutoGen&#x27;s TeachableAgent</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/10/18/RetrieveChat">Retrieval-Augmented Generation (RAG) Applications with AutoGen</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/07/14/Local-LLMs">Use AutoGen for Local LLMs</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;LMM&quot;</h1><a href="/autogen/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/autogen/blog/2023/11/09/EcoAssistant">EcoAssistant - Using LLM Assistants More Accurately and Affordably</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-11-09T00:00:00.000Z" itemprop="datePublished">November 9, 2023</time> Â· <!-- -->5 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://jieyuz2.github.io/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/jieyuz2.png" alt="Jieyu Zhang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://jieyuz2.github.io/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jieyu Zhang</span></a></div><small class="avatar__subtitle" itemprop="description">PhD student at University of Washington</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="system" src="/autogen/assets/images/system-1f6d283e77b49cac460bb999adb6bd5d.png"></p><p><strong>TL;DR:</strong></p><ul><li>Introducing the <strong>EcoAssistant</strong>, which is designed to solve user queries more accurately and affordably.</li><li>We show how to let the LLM assistant agent leverage external API to solve user query.</li><li>We show how to reduce the cost of using GPT models via <strong>Assistant Hierachy</strong>.</li><li>We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via <strong>Solution Demonstration</strong>.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ecoassistant">EcoAssistant<a aria-hidden="true" class="hash-link" href="#ecoassistant" title="Direct link to heading">â€‹</a></h2><p>In this blog, we introduce the <strong>EcoAssistant</strong>, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="problem-setup">Problem setup<a aria-hidden="true" class="hash-link" href="#problem-setup" title="Direct link to heading">â€‹</a></h3><p>Recently, users have been using conversational LLMs such as ChatGPT for various queries.
Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.
Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).
These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.
In the table below, we show three types of user queries that we aim to address in this work.</p><table><thead><tr><th>Dataset</th><th>API</th><th>Example query</th></tr></thead><tbody><tr><td>Places</td><td><a href="https://developers.google.com/maps/documentation/places/web-service/overview" target="_blank" rel="noopener noreferrer">Google Places</a></td><td>Iâ€™m looking for a 24-hour pharmacy in Montreal, can you find one for me?</td></tr><tr><td>Weather</td><td><a href="https://www.weatherapi.com" target="_blank" rel="noopener noreferrer">Weather API</a></td><td>What is the current cloud coverage in Mumbai, India?</td></tr><tr><td>Stock</td><td><a href="https://www.alphavantage.co/documentation/" target="_blank" rel="noopener noreferrer">Alpha Vantage Stock API</a></td><td>Can you give me the opening price of Microsoft for the month of January 2023?</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_y2LR" id="leveraging-external-apis">Leveraging external APIs<a aria-hidden="true" class="hash-link" href="#leveraging-external-apis" title="Direct link to heading">â€‹</a></h3><p>To address these queries, we first build a <strong>two-agent system</strong> based on AutoGen,
where the first agent is a <strong>LLM assistant agent</strong> (<code>AssistantAgent</code> in AutoGen) that is responsible for proposing and refining the code and
the second agent is a <strong>code executor agent</strong> (<code>UserProxyAgent</code> in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.
A visualization of the two-agent system is shown below.</p><p><img alt="chat" src="/autogen/assets/images/chat-a2adea6a92b3cd4059021840c869d7d5.png"></p><p>To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.
The template is shown below, where the red part is the information of APIs and black part is user query.</p><p><img alt="template" src="/autogen/assets/images/template-c610ae53eaa7afa3adaf670fa74b5c10.png"></p><p>Importantly, we don&#x27;t want to reveal our real API key to the assistant agent for safety concerns.
Therefore, we use a <strong>fake API key</strong> to replace the real API key in the initial message.
In particular, we generate a random token (e.g., <code>181dbb37</code>) for each API key and replace the real API key with the token in the initial message.
Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="solution-demonstration">Solution Demonstration<a aria-hidden="true" class="hash-link" href="#solution-demonstration" title="Direct link to heading">â€‹</a></h3><p>In most practical scenarios, queries from users would appear sequentially over time.
Our <strong>EcoAssistant</strong> leverages past success to help the LLM assistants address future queries via <strong>Solution Demonstration</strong>.
Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.
These query-code pairs are saved in a specialized vector database. When new queries appear, <strong>EcoAssistant</strong> retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.
The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.</p><p><img alt="template" src="/autogen/assets/images/template-demo-5a8cae3df56acdcf73188e401ad739f5.png"></p><p>We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system&#x27;s performance.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="assistant-hierarchy">Assistant Hierarchy<a aria-hidden="true" class="hash-link" href="#assistant-hierarchy" title="Direct link to heading">â€‹</a></h3><p>LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.
Thus, we propose the <strong>Assistant Hierarchy</strong> to reduce the cost of using LLMs.
The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.
By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.
In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.
If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, <strong>EcoAssistant</strong> would then restart the conversation with the next more expensive LLM assistant in the hierarchy.
We found that this strategy significantly reduces costs while still effectively addressing queries.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="a-synergistic-effect">A Synergistic Effect<a aria-hidden="true" class="hash-link" href="#a-synergistic-effect" title="Direct link to heading">â€‹</a></h3><p>We found that the <strong>Assistant Hierarchy</strong> and <strong>Solution Demonstration</strong> of <strong>EcoAssistant</strong> have a synergistic effect.
Because the query-code database is shared by all LLM assistants, even without specialized design,
the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).
Such a synergistic effect further improves the performance and reduces the cost of <strong>EcoAssistant</strong>.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="experimental-results">Experimental Results<a aria-hidden="true" class="hash-link" href="#experimental-results" title="Direct link to heading">â€‹</a></h3><p>We evaluate <strong>EcoAssistant</strong> on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that <strong>EcoAssistant</strong> achieves a higher success rate with a lower cost as shown in the figure below.
For more details about the experimental results and other experiments, please refer to our <a href="https://arxiv.org/abs/2310.03046" target="_blank" rel="noopener noreferrer">paper</a>.</p><p><img alt="exp" src="/autogen/assets/images/results-4c8cfbb728760a85ce2d549fd7798179.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="further-reading">Further reading<a aria-hidden="true" class="hash-link" href="#further-reading" title="Direct link to heading">â€‹</a></h2><p>Please refer to our <a href="https://arxiv.org/abs/2310.03046" target="_blank" rel="noopener noreferrer">paper</a> and <a href="https://github.com/JieyuZ2/EcoAssistant" target="_blank" rel="noopener noreferrer">codebase</a> for more details about <strong>EcoAssistant</strong>.</p><p>If you find this blog useful, please consider citing:</p><div class="codeBlockContainer_J+bg language-bibtex"><div class="codeBlockContent_csEI bibtex"><pre tabindex="0" class="prism-code language-bibtex codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@article{zhang2023ecoassistant,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  journal={arXiv preprint arXiv:2310.03046},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  year={2023}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/lmm">LMM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/rag">RAG</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/cost-effectiveness">cost-effectiveness</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about EcoAssistant - Using LLM Assistants More Accurately and Affordably" href="/autogen/blog/2023/11/09/EcoAssistant"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-11-06T00:00:00.000Z" itemprop="datePublished">November 6, 2023</time> Â· <!-- -->3 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/beibinli" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/beibinli.png" alt="Beibin Li"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/beibinli" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Beibin Li</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Research Engineer at Microsoft</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="LMM Teaser" src="/autogen/assets/images/teaser-380bdaa90a1c02ad009520bf289776c9.png"></p><p><strong>In Brief:</strong></p><ul><li>Introducing the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> to enhance LMM functionalities.</li><li>Users can input text and images simultaneously using the <code>&lt;img img_path&gt;</code> tag to specify image loading.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb" target="_blank" rel="noopener noreferrer">GPT-4V notebook</a>.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb" target="_blank" rel="noopener noreferrer">LLaVA notebook</a>.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="introduction">Introduction<a aria-hidden="true" class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h2><p>Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.</p><p>This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.
We support the <code>gpt-4-vision-preview</code> model from OpenAI and <code>LLaVA</code> model from Microsoft now.</p><p>Here, we emphasize the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> due to their growing popularity.
GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="installation">Installation<a aria-hidden="true" class="hash-link" href="#installation" title="Direct link to heading">â€‹</a></h2><p>Incorporate the <code>lmm</code> feature during AutoGen installation:</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip </span><span class="token function" style="color:rgb(130, 170, 255)">install</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;pyautogen[lmm]&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Subsequently, import the <strong>Multimodal Conversable Agent</strong> or <strong>LLaVA Agent</strong> from AutoGen:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">multimodal_conversable_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> MultimodalConversableAgent  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># for GPT-4V</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">llava_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> LLaVAAgent  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># for LLaVA</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="usage">Usage<a aria-hidden="true" class="hash-link" href="#usage" title="Direct link to heading">â€‹</a></h2><p>A simple syntax has been defined to incorporate both messages and images within a single string.</p><p>Example of an in-context learning prompt:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are now an image classifier for facial expressions. Here are</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">some examples.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img happy.jpg&gt; depicts a happy expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img http://some_location.com/sad.jpg&gt; represents a sad expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img obama.jpg&gt; portrays a neutral expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">Now, identify the facial expression of this individual: &lt;img unknown.png&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">agent </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> MultimodalConversableAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">agent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The <code>MultimodalConversableAgent</code> interprets the input prompt, extracting images from local or internet sources.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="advanced-usage">Advanced Usage<a aria-hidden="true" class="hash-link" href="#advanced-usage" title="Direct link to heading">â€‹</a></h2><p>Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.</p><p>For example, the <code>FigureCreator</code> in our <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb" target="_blank" rel="noopener noreferrer">GPT-4V notebook</a> and <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb" target="_blank" rel="noopener noreferrer">LLaVA notebook</a> integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).
The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.
With <code>human_input_mode=ALWAYS</code>, you can also contribute suggestions for better visualizations.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="reference">Reference<a aria-hidden="true" class="hash-link" href="#reference" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://openai.com/research/gpt-4v-system-card" target="_blank" rel="noopener noreferrer">GPT-4V System Card</a></li><li><a href="https://github.com/haotian-liu/LLaVA" target="_blank" rel="noopener noreferrer">LLaVA GitHub</a></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="future-enhancements">Future Enhancements<a aria-hidden="true" class="hash-link" href="#future-enhancements" title="Direct link to heading">â€‹</a></h2><p>For further inquiries or suggestions, please open an issue in the <a href="https://github.com/microsoft/autogen/" target="_blank" rel="noopener noreferrer">AutoGen repository</a> or contact me directly at <a href="mailto:beibin.li@microsoft.com" target="_blank" rel="noopener noreferrer">beibin.li@microsoft.com</a>.</p><p>AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/lmm">LMM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/multimodal">multimodal</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Multimodal with GPT-4V and LLaVA" href="/autogen/blog/2023/11/06/LMM-Agent"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 AutoGen Authors.</div></div></div></footer></div>
<script src="/autogen/assets/js/runtime~main.5370b1fb.js"></script>
<script src="/autogen/assets/js/main.690578dc.js"></script>
</body>
</html>